{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "    Minigratch Gradient Descent Function to train model\n",
    "    1. Format the data\n",
    "    2. call four_nn function to obtain losses\n",
    "    3. Return all the weights/biases and a list of losses at each epoch\n",
    "    Args:\n",
    "        epoch (int) - number of iterations to run through neural net\n",
    "        w1, w2, w3, w4, b1, b2, b3, b4 (numpy arrays) - starting weights\n",
    "        x_train (np array) - (n,d) numpy array where d=number of features\n",
    "        y_train (np array) - (n,) all the labels corresponding to x_train\n",
    "        num_classes (int) - number of classes (range of y_train)\n",
    "        shuffle (bool) - shuffle data at each epoch if True. Turn this off for testing.\n",
    "    Returns:\n",
    "        w1, w2, w3, w4, b1, b2, b3, b4 (numpy arrays) - resulting weights\n",
    "        losses (list of ints) - each index should correspond to epoch number\n",
    "            Note that len(losses) == epoch\n",
    "    Hints:\n",
    "        Should work for any number of features and classes\n",
    "        Good idea to print the epoch number at each iteration for sanity checks!\n",
    "        (Stdout print will not affect autograder as long as runtime is within limits)\n",
    "\"\"\"\n",
    "def minibatch_gd(epoch, w1, w2, w3, w4, b1, b2, b3, b4, x_train, y_train, num_classes, shuffle=True):\n",
    "    batch_size = 200\n",
    "    num_batches = int(len(x_train)/batch_size)\n",
    "    losses = [0 for x in range(epoch)]\n",
    "\n",
    "    for i in range(epoch):\n",
    "\n",
    "        print(\"Epoch: \", i+1)\n",
    "        loss = 0\n",
    "\n",
    "        if (shuffle == True):\n",
    "            idx = np.random.choice(len(x_train), len(x_train), False)\n",
    "            x = x_train[idx]\n",
    "            y = y_train[idx]\n",
    "\n",
    "        else:\n",
    "            x = x_train.copy()\n",
    "            y = y_train.copy()\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            x_test = x[j*batch_size : (j+1)*batch_size]\n",
    "            y_test = y[j*batch_size : (j+1)*batch_size]\n",
    "            loss += four_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, num_classes, False)\n",
    "        \n",
    "        losses[i] = loss\n",
    "    \n",
    "    return w1, w2, w3, w4, b1, b2, b3, b4, losses\n",
    "\n",
    "\"\"\"\n",
    "    Use the trained weights & biases to see how well the nn performs\n",
    "        on the test data\n",
    "    Args:\n",
    "        All the weights/biases from minibatch_gd()\n",
    "        x_test (np array) - (n', d) numpy array\n",
    "        y_test (np array) - (n',) all the labels corresponding to x_test\n",
    "        num_classes (int) - number of classes (range of y_test)\n",
    "    Returns:\n",
    "        avg_class_rate (float) - average classification rate\n",
    "        class_rate_per_class (list of floats) - Classification Rate per class\n",
    "            (index corresponding to class number)\n",
    "    Hints:\n",
    "        Good place to show your confusion matrix as well.\n",
    "        The confusion matrix won't be autograded but necessary in report.\n",
    "\"\"\"\n",
    "def test_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, num_classes):\n",
    "    class_rate_per_class = [0.0] * num_classes\n",
    "    classifications = four_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, num_classes, True)\n",
    "    avg_class_rate = np.sum(y_test == classifications)/len(y_test)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_rate_per_class[i] = np.sum(classifications[np.argwhere(y_test == i)] == i)/len(np.argwhere(y_test == i))\n",
    "\n",
    "    return avg_class_rate, class_rate_per_class,classifications\n",
    "\n",
    "\"\"\"\n",
    "    4 Layer Neural Network\n",
    "    Helper function for minibatch_gd\n",
    "    Up to you on how to implement this, won't be unit tested\n",
    "    Should call helper functions below\n",
    "\"\"\"\n",
    "def four_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, num_classes, test):\n",
    "    Z1, acache1 = affine_forward(x_test, w1, b1)\n",
    "    A1, rcache1 = relu_forward(Z1)\n",
    "    Z2, acache2 = affine_forward(A1, w2, b2)\n",
    "    A2, rcache2 = relu_forward(Z2)\n",
    "    Z3, acache3 = affine_forward(A2, w2, b2)\n",
    "    A3, rcache3 = relu_forward(Z3)\n",
    "    F, acache4 = affine_forward(A3, w4, b4)\n",
    "\n",
    "    if test:\n",
    "        return np.argmax(F, axis=1)\n",
    "\n",
    "    else:\n",
    "        eta = 0.1\n",
    "        loss, dF = cross_entropy(F, y_test)\n",
    "\n",
    "        dA3, dW4, db4 = affine_backward(dF, acache4)\n",
    "        dZ3 = relu_backward(dA3, rcache3)\n",
    "        dA2, dW3, db3 = affine_backward(dZ3, acache3)\n",
    "        dZ2 = relu_backward(dA2, rcache2)\n",
    "        dA1, dW2, db2 = affine_backward(dZ2, acache2)\n",
    "        dZ1 = relu_backward(dA1, rcache1)\n",
    "        dX, dW1, db1 = affine_backward(dZ1, acache1)\n",
    "\n",
    "        w1 -= eta*dW1\n",
    "        w2 -= eta*dW2\n",
    "        w3 -= eta*dW3\n",
    "        w4 -= eta*dW4\n",
    "        b1 -= eta*db1\n",
    "        b2 -= eta*db2\n",
    "        b3 -= eta*db3\n",
    "        b4 -= eta*db4\n",
    "\n",
    "        return loss\n",
    "\n",
    "\"\"\"\n",
    "    Next five functions will be used in four_nn() as helper functions.\n",
    "    All these functions will be autograded, and a unit test script is provided as unit_test.py.\n",
    "    The cache object format is up to you, we will only autograde the computed matrices.\n",
    "\n",
    "    Args and Return values are specified in the MP docs\n",
    "    Hint: Utilize numpy as much as possible for max efficiency.\n",
    "        This is a great time to review on your linear algebra as well.\n",
    "\"\"\"\n",
    "def affine_forward(A, W, b):\n",
    "    return np.matmul(A,W)+b, (A, W)\n",
    "\n",
    "def affine_backward(dZ, cache):\n",
    "    dA = np.matmul(dZ, cache[1].T)\n",
    "    dW = np.matmul(cache[0].T, dZ)\n",
    "    dB = np.sum(dZ, axis=0)\n",
    "    return dA, dW, dB\n",
    "\n",
    "def relu_forward(Z):\n",
    "    A = Z.copy()\n",
    "    A[A<0] = 0\n",
    "    return A, Z\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    dA[np.where(cache<0)] = 0\n",
    "    return dA\n",
    "\n",
    "def cross_entropy(F, y):\n",
    "    loss = -(1/len(F))*np.sum(F[np.arange(len(F)), y.astype(int)] - np.log(np.sum(np.exp(F), axis=1)))\n",
    "    class_label = np.zeros(F.shape)\n",
    "    class_label[np.arange(len(F)), y.astype(int)] = 1  \n",
    "    dF = -(1/len(F))*(class_label - np.exp(F) / (np.sum(np.exp(F), axis=1)).reshape((-1, 1)) )\n",
    "\n",
    "    return loss, dF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'minibatch_gd' from 'neuralnet' (/Users/xuke/Documents/GitHub/ECE-448-Artificial-Intelligence/MPs/MP5/part 2/neuralnet.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vh/vy1h_x1105q5pl_wdj84g_140000gn/T/ipykernel_41586/3981730702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuralnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminibatch_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'minibatch_gd' from 'neuralnet' (/Users/xuke/Documents/GitHub/ECE-448-Artificial-Intelligence/MPs/MP5/part 2/neuralnet.py)"
     ]
    }
   ],
   "source": [
    "from neuralnet import minibatch_gd, test_nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import time\n",
    "\n",
    "def init_weights(d, dp):\n",
    "    return 0.01 * np.random.uniform(0.0, 1.0, (d, dp)), np.zeros(dp)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_train = np.load(\"data/x_train.npy\")\n",
    "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "    y_train = np.load(\"data/y_train.npy\")\n",
    "\n",
    "    x_test = np.load(\"data/x_test.npy\")\n",
    "    x_test = (x_test - np.mean(x_test, axis=0))/np.std(x_test, axis=0)\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "\n",
    "    load_weights = False #set to True if you want to use saved weights\n",
    "\n",
    "    if load_weights:\n",
    "        w1 = np.load('w1')\n",
    "        w2 = np.load('w2')\n",
    "        w3 = np.load('w3')\n",
    "        w4 = np.load('w4')\n",
    "\n",
    "        b1 = np.load('b1')\n",
    "        b2 = np.load('b2')\n",
    "        b3 = np.load('b3')\n",
    "        b4 = np.load('b4')\n",
    "    else:\n",
    "        w1, b1 = init_weights(784, 256)\n",
    "        w2, b2 = init_weights(256, 256)\n",
    "        w3, b3 = init_weights(256, 256)\n",
    "        w4, b4 = init_weights(256, 10)\n",
    "\n",
    "    time_start=time.time()\n",
    "    w1, w2, w3, w4, b1, b2, b3, b4, losses = minibatch_gd(30, w1, w2, w3, w4, b1, b2, b3, b4, x_train, y_train, 10)\n",
    "    time_end=time.time()\n",
    "    \n",
    "    np.save('w1', w1)\n",
    "    np.save('w2', w2)\n",
    "    np.save('w3', w3)\n",
    "    np.save('w4', w4)\n",
    "\n",
    "    np.save('b1', b1)\n",
    "    np.save('b2', b2)\n",
    "    np.save('b3', b3)\n",
    "    np.save('b4', b4)\n",
    "\n",
    "    avg_class_rate, class_rate_per_class, y_pred = test_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, 10)\n",
    "\n",
    "    print(avg_class_rate, class_rate_per_class)\n",
    "    print('Time:', time_end-time_start,'s')\n",
    "\n",
    "    class_names = np.array([\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x_train = np.load(\"data/x_train.npy\")\n",
    "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "    y_train = np.load(\"data/y_train.npy\")\n",
    "\n",
    "    x_test = np.load(\"data/x_test.npy\")\n",
    "    x_test = (x_test - np.mean(x_test, axis=0))/np.std(x_test, axis=0)\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "\n",
    "    load_weights = False #set to True if you want to use saved weights\n",
    "\n",
    "    if load_weights:\n",
    "        w1 = np.load('w1')\n",
    "        w2 = np.load('w2')\n",
    "        w3 = np.load('w3')\n",
    "        w4 = np.load('w4')\n",
    "\n",
    "        b1 = np.load('b1')\n",
    "        b2 = np.load('b2')\n",
    "        b3 = np.load('b3')\n",
    "        b4 = np.load('b4')\n",
    "    else:\n",
    "        w1, b1 = init_weights(784, 256)\n",
    "        w2, b2 = init_weights(256, 256)\n",
    "        w3, b3 = init_weights(256, 256)\n",
    "        w4, b4 = init_weights(256, 10)\n",
    "\n",
    "    time_start=time.time()\n",
    "    w1, w2, w3, w4, b1, b2, b3, b4, losses = minibatch_gd(10, w1, w2, w3, w4, b1, b2, b3, b4, x_train, y_train, 10)\n",
    "    time_end=time.time()\n",
    "    \n",
    "    np.save('w1', w1)\n",
    "    np.save('w2', w2)\n",
    "    np.save('w3', w3)\n",
    "    np.save('w4', w4)\n",
    "\n",
    "    np.save('b1', b1)\n",
    "    np.save('b2', b2)\n",
    "    np.save('b3', b3)\n",
    "    np.save('b4', b4)\n",
    "\n",
    "    avg_class_rate, class_rate_per_class, y_pred = test_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, 10)\n",
    "\n",
    "    print(avg_class_rate, class_rate_per_class)\n",
    "    print('Time:', time_end-time_start,'s')\n",
    "\n",
    "    class_names = np.array([\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x_train = np.load(\"data/x_train.npy\")\n",
    "    x_train = (x_train - np.mean(x_train, axis=0)) / np.std(x_train, axis=0)\n",
    "    y_train = np.load(\"data/y_train.npy\")\n",
    "\n",
    "    x_test = np.load(\"data/x_test.npy\")\n",
    "    x_test = (x_test - np.mean(x_test, axis=0))/np.std(x_test, axis=0)\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "\n",
    "    load_weights = False #set to True if you want to use saved weights\n",
    "\n",
    "    if load_weights:\n",
    "        w1 = np.load('w1')\n",
    "        w2 = np.load('w2')\n",
    "        w3 = np.load('w3')\n",
    "        w4 = np.load('w4')\n",
    "\n",
    "        b1 = np.load('b1')\n",
    "        b2 = np.load('b2')\n",
    "        b3 = np.load('b3')\n",
    "        b4 = np.load('b4')\n",
    "    else:\n",
    "        w1, b1 = init_weights(784, 256)\n",
    "        w2, b2 = init_weights(256, 256)\n",
    "        w3, b3 = init_weights(256, 256)\n",
    "        w4, b4 = init_weights(256, 10)\n",
    "\n",
    "    time_start=time.time()\n",
    "    w1, w2, w3, w4, b1, b2, b3, b4, losses = minibatch_gd(50, w1, w2, w3, w4, b1, b2, b3, b4, x_train, y_train, 10)\n",
    "    time_end=time.time()\n",
    "    \n",
    "    np.save('w1', w1)\n",
    "    np.save('w2', w2)\n",
    "    np.save('w3', w3)\n",
    "    np.save('w4', w4)\n",
    "\n",
    "    np.save('b1', b1)\n",
    "    np.save('b2', b2)\n",
    "    np.save('b3', b3)\n",
    "    np.save('b4', b4)\n",
    "\n",
    "    avg_class_rate, class_rate_per_class, y_pred = test_nn(w1, w2, w3, w4, b1, b2, b3, b4, x_test, y_test, 10)\n",
    "\n",
    "    print(avg_class_rate, class_rate_per_class)\n",
    "    print('Time:', time_end-time_start,'s')\n",
    "\n",
    "    class_names = np.array([\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "\n",
    "    x = np.arange(1,51,1)\n",
    "    plt.plot(x,losses)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
